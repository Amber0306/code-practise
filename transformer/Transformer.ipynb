{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "#### CNN\n",
    "+ 权重共享 平移不变性、可并行计算\n",
    "+ 滑动窗口 局部关联性建模、依靠多层堆积来进行长程建模\n",
    "+ 对相对位置敏感、对绝对位置不敏感\n",
    "\n",
    "#### RNN 依次有序递归建模\n",
    "+ 对顺序敏感\n",
    "+ 串行计算耗时\n",
    "+ 长程建模能力弱\n",
    "+ 计算复杂度与序列长度呈线性关系\n",
    "+ 单步计算复杂度不变\n",
    "+ 对相对位置敏感，对绝对位置敏感\n",
    "  \n",
    "#### tranformer\n",
    "+ 无局部假设\n",
    "  + 可并行计算\n",
    "  + 对相对位置不敏感\n",
    "+ 无有序假设\n",
    "  + 需要位置编码反映位置变化对特征的影响\n",
    "  + 对绝对位置不敏感\n",
    "+ 任意两字符都可以建模\n",
    "  + 擅长长短程建模\n",
    "  + 自注意力机制需要序列长度的平方级复杂运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer encoder\n",
    "+ input word embedding\n",
    "  由稀疏的one-hot进入一个不带bias的FFN得到一个稠密的连续箱梁\n",
    "+ position encoding\n",
    "  + 通过sin/cos来固定表征\n",
    "    + 每个位置确定\n",
    "    + 对于不同的句子，相同位置的距离一致\n",
    "    + 可以推广到更长的测试句子\n",
    "  + pe(pos+k)可以写成pe(k)的线性组合\n",
    "  + 通过残差连接来是的位置信息流入深层\n",
    "+ multi-head self-attention\n",
    "  + 建模能力更强，表征空间更丰富\n",
    "  + 由多组Q\\K\\V构成，每组单独计算一个attention向量\n",
    "  + 把每组的attention向量拼起来，并进入一个FFN中得到最终的向量\n",
    "+ feed-forward network\n",
    "  + 只考虑每个单独位置进行建模\n",
    "  + 不同位置参数共享\n",
    "  + 类似于1*1 pointwise convolution\n",
    "\n",
    "### Transformer Decoder\n",
    "+ output word embedding\n",
    "+ masked multi-head self-attention\n",
    "+ multi-head cross-attention\n",
    "+ feed-forward network\n",
    "+ softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from .. import functional as F"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
