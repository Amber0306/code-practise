{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform工具箱\n",
    "toTensor，，resize等工具\n",
    "\n",
    "通过transform.ToTensor去解决两个问题\n",
    "- 1.tranforms该如何使用\n",
    "- 2.tensor数据类型的不同之处\n",
    "  \n",
    "工具创建具体的工具，使用工具：输入图片，输出处理后图片\n",
    "result = tool(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1184 at 0x13CC47ADF60>\n"
     ]
    }
   ],
   "source": [
    "image_path = 'E:\\share\\code-practise\\pics\\pxh.jpg'\n",
    "img = Image.open(image_path)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-abe646fa7085>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtensor_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtensor_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_trans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "tensor_trans = transforms.ToTensor()\n",
    "tensor_img = tensor_trans(img)\n",
    "print(tensor_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输入\n",
    "  - PIL Image.open()\n",
    "  - tensor ToTensor()\n",
    "  - narrays cv.imread()\n",
    "- 输出\n",
    "- 作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1184 at 0x2D7B217C470>\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"../../pics/pxh.jpg\")\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__cal__amber\n",
      "hello lisi\n"
     ]
    }
   ],
   "source": [
    "# test call\n",
    "class Person:\n",
    "    def __call__(self,name):\n",
    "        print(\"__cal__\"+name)\n",
    "    \n",
    "    def hello(self,name):\n",
    "        print(\"hello \"+name)\n",
    "        \n",
    "person = Person()\n",
    "person(\"amber\")\n",
    "person.hello(\"lisi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "trans_totensor = transforms.ToTensor()\n",
    "img_tensor = trans_totensor(img)\n",
    "writer.add_image(\"totensor\",img_tensor)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1059)\n",
      "tensor(-0.2980)\n"
     ]
    }
   ],
   "source": [
    "# noramaliza\n",
    "print(img_tensor[0][0][0])\n",
    "trans_norm = transforms.Normalize([1,3,5],[3,2,1])\n",
    "img_norm = trans_norm(img_tensor)\n",
    "print(img_norm[0][0][0])\n",
    "writer.add_image('Normalize',img_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1184)\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2D7B333D780>\n"
     ]
    }
   ],
   "source": [
    "# resize\n",
    "print(img.size)\n",
    "trans_resize = transforms.Resize((512,512))\n",
    "img_resize = trans_resize(img)\n",
    "print(img_resize)\n",
    "img_resize = trans_totensor(img_resize)\n",
    "writer.add_image(\"resize\",img_resize,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose\n",
    "trans_resize_2 = transforms.Resize(512)\n",
    "trans_compose = transforms.Compose([trans_resize_2,trans_totensor])\n",
    "img_resize_2 = trans_compose(img)\n",
    "writer.add_image(\"resize\",img_resize_2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c5745817566791c0a4caeccd7953bb9b74969c1af50fefea7ebb4e20ec995e7"
  },
  "kernelspec": {
   "display_name": "Python36-pytorch-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
